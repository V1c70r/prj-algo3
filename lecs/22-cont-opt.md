# Непрерывная оптимизация

Непрерывная оптимизация — это алгоритмы итеративного решения задачи минимизации непрерывной функции, которую невозможно или непрактично решать аналитически.

## Градиентный спуск

Метод градиентного спуска заключается в последовательном изменении параметров функции в направлении ее убывания, которое является противоположным к ее градиенту (производной). Величина шага оптимизации зависит от величины градиента, задаваемого пользователем параметра скорости обучения (как правило, в диапазоне от 0 до 1 - например, 0.1), а также, возможно, других параметров (для оптимизированных методов).

Варианты градиентного спуска (GD):

- батч и минибатч GD
- SGD (стохастический градиентный спуск)
- GD с моментом
- NAG (ускоренный градиент Нестерова)
- RMSProp
- Adagrad
- Adadelta
- Adam

## Обратное распространение ошибки

Метод обратного распространения ошибки (backprop) — это алгоритм эффективного вычисления значений градиента в сложных вычислительных графах (которые представляют сложные функции от многих параметров — например, нейронные сети). Он использует мемоизацию для того, чтобы вычислять градиент на каждом ребре графа только 1 раз.


## Литература

- https://hackernoon.com/life-is-gradient-descent-880c60ac1be8
- https://web.archive.org/web/20151122203025/http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf
- http://sebastianruder.com/optimizing-gradient-descent/
- https://github.com/mazefeng/sgd-opt
- http://www.math.ucla.edu/~biskup/164.2.14f/PDFs/recursions.pdf
- http://colah.github.io/posts/2015-08-Backprop/
- https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
